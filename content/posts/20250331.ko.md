---
title: "퍼징 가이드 정적 분석: 강화학습 관점에서"
date: 2025-03-31
---

현대 퍼징 기법(예: 방향성 퍼징)은 퍼징 캠페인을 안내하기 위해 정적 분석 정보에 크게 의존합니다. 하지만 정적 분석이 부정확한 결과를 생성할 때, 종종 인간이 설계한 휴리스틱에 의존하기 때문에 이러한 캠페인의 성공이 크게 제한될 수 있습니다.

핵심 통찰은 퍼징 결과가 프로그램 동작에 대한 실제 정보를 제공한다는 것입니다. 이 정보는 두 가지 방법으로 정적 분석을 개선하는 데 도움이 될 수 있습니다: 불완전한 분석으로 인해 잃어버린 정보를 복구하고, 지나치게 보수적인 분석에서 나온 가짜 결과를 제거하는 것입니다. 이러한 양방향 관계는 정적 분석과 동적 분석 사이의 긍정적인 피드백 루프를 만듭니다.

강화학습 관점에서, 정적 분석 결과는 실제 환경인 구체적인 프로그램의 모델 역할을 합니다. 퍼저는 이 모델을 바탕으로 적절한 정책(예: 선택적 커버리지, 에너지 분배)을 선택하여 퍼징을 수행합니다. 충분한 퍼징 결과를 수집한 후, 그 정보를 바탕으로 모델(정적 분석 결과)을 개선할 수 있습니다. 개선된 모델을 사용한 퍼징은 이전보다 더 효율적일 것입니다.

이 문제는 모델 기반, 오프 정책 강화학습 문제로 공식화할 수 있습니다. 이러한 공식화는 정적 분석 없이 퍼징 과정 자체를 주로 고려하는 기존의 강화학습 기반 퍼징 연구와는 명확히 다릅니다. 기존 강화학습 분야의 지식과 퍼징 및 정적 분석의 통찰을 결합함으로써 이 문제를 해결할 수 있다고 믿습니다. 